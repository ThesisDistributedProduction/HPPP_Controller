<?xml version="1.0" encoding="UTF-8"?>

<dds>
    <!--qos_library name="DefaultLibrary" -->
    <qos_library name="Turbine_Library">
        <!--qos_profile name="ParticipantQOS" is_default_participant_factory_profile="true"-->
        <qos_profile name="Turbine_Profile" is_default_qos="true">

            <participant_factory_qos>
				<!-- relevant for siemens -->
                <logging>
                    <verbosity>WARNING</verbosity>
                    <category>PLATFORM</category>
                    <print_format>VERBOSE_TIMESTAMPED</print_format>
                    <output_file>ddsadaptor.log</output_file>
                </logging>
            </participant_factory_qos>

            <participant_qos>
                <participant_name>
                    <name>TurbineParticipant</name>
                    <role_name>TurbineParticipantRole</role_name>
                </participant_name>
				<!-- db cleanup after shutdown of a participant. Db is the db local and remote entities -->
                <database>
                    <shutdown_cleanup_period>
                        <sec>DURATION_ZERO_SEC</sec>
                        <nanosec>1</nanosec>
                    </shutdown_cleanup_period>
                </database>
                <transport_builtin>
                     <!--
                         The transport_builtin mask identifies which builtin
                         transports the domain participant uses. The default value
                         is UDPv4 | SHMEM.
                     -->
                    <mask>UDPv4</mask>
                </transport_builtin>
                <receiver_pool>
                    <buffer_size>LENGTH_AUTO</buffer_size>
                </receiver_pool>
                <property>
                    <value>
                        <!--element>
                            <name>dds.transport.UDPv4.builtin.multicast_enabled</name>
                            <value>0</value>
                        </element -->
						<element>
							<name>dds.transport.UDPv4.builtin.parent.message_size_max</name>
							<value>65535</value>
						</element>
						<element>
							<name>dds.transport.UDPv4.builtin.send_socket_buffer_size </name>
							<value>65535</value>
						</element>
						<element>
							<name>dds.transport.UDPv4.builtin.recv_socket_buffer_size</name>
							<value>2097152</value>
						</element>
					</value>
                </property>
            </participant_qos>
			
			<topic_qos name="TurbineTopicQos">
				<!-- 
				 DDS_PROTOCOL_ACKNOWLEDGMENT_MODE (Default) = No application-level acknowledgement. 
				 AckNack sker gennem Real-Time Publish-Subscribe (RTPS) reliability protocol. -->
				<reliability>
				  <kind>DDS_RELIABLE_RELIABILITY_QOS</kind>
				</reliability>
			</topic_qos>
			
			<datawriter_qos>
				<publication_name>
					<name>TurbineDataWriter</name>
				</publication_name>
				
				<!-- StÃ¸rrelse af reliability buffer -->
				<history>
					<kind>KEEP_ALL_HISTORY_QOS</kind>
				</history>
				
                <!-- HIGH THROUGHPUT
                We're limiting resources based on the number of batches. We
                could limit resources on a per-sample basis too if we wanted
                to; we'd probably to set the value based on how many samples
                we expect to be in each batch. Rather than come up with a
                heuristic, however, it's more straightforward to override
                this value to leave the value unlimited. (If you were to set
                both, the first limit to be hit would take effect.)
                
                <resource_limits>
                    <max_samples>LENGTH_UNLIMITED</max_samples>
                </resource_limits>-->
				<writer_resource_limits>
                    <!--
                    The number of batches (not samples) for which the
                    DataWriter will allocate space.

                    The initial_batches parameter is also set here, indicating
                    to the writer that it should pre-allocate all of the space
                    up front. Pre-allocation will remove memory allocattion
                    from the critical path of the application, improving
                    performance and determinism.

                    Finite resources are not required for strict reliability.
                    However, by limiting how far "ahead" of its readers a
                    writer is able to get, you can make the system more
                    robust and performant in the face of slow readers and/or
                    dropped packets while at the same time constraining your
                    memory growth.
                    -->
                    <max_batches>32</max_batches>
                    <initial_batches>32</initial_batches>
                </writer_resource_limits>
				<resource_limits>
                    <max_samples>LENGTH_UNLIMITED</max_samples>
                    <!--max_samples_per_instance>20</max_samples_per_instance-->
                </resource_limits>
                <reliability>
                    <max_blocking_time>
                        <sec>5</sec>
                        <nanosec>0</nanosec>
                    </max_blocking_time>
                </reliability>
				<!-- <lifespan>
					<duration>
						 <sec>0</sec>
						 <nanosec>150000000</nanosec>
					</duration>
				</lifespan> -->
				
				<!-- <liveliness>
					<lease_duration>
						<sec>0</sec>
						<nanosec>300000000</nanosec>
					</lease_duration>
				</liveliness> --> 
				
				<!--
                The following parameters tune the behavior of the reliability
                protocol. Setting them is not required in order to achieve
                strict reliability but is beneficial from a performance
                standpoint. 
                -->
                <protocol>
                    <rtps_reliable_writer>
                        <!--
                        When the writer's cache gets down to this number of
                        samples, it will slow the rate at which it sends
                        heartbeats to readers.
                        -->
                        <low_watermark>5</low_watermark>
                        <!--
                        When the writer's cache is filled to this level, it
                        will begin sending heartbeats at a faster rate in
                        order to spur faster acknowledgement (positive or
                        negative) of its samples to allow it to empty its
                        cache and avoid blocking.
                        -->
                        <high_watermark>15</high_watermark>

                        <!--
                        If the number of samples in the writer's cache hasn't
                        risen to high_watermark, this is the rate at which
                        the DataWriter will send out periodic heartbeats.
                        -->
                        <heartbeat_period>
                            <!-- 100 milliseconds: -->
                            <sec>0</sec>
                            <nanosec>100000000</nanosec>
                        </heartbeat_period>
                        <!--
                        If the number of samples in the writer's cache has
                        risen to high_watermark, and has not yet fallen to
                        low_watermark, this is the rate at which the writer
                        will send periodic heartbeats to its readers.
                        -->
                        <fast_heartbeat_period>
                            <!-- 10 milliseconds: -->
                            <sec>0</sec>
                            <nanosec>10000000</nanosec>
                        </fast_heartbeat_period>
                        <!--
                        If a durable reader starts up after the writer
                        already has some samples in its cache, this is the
                        rate at which it will heartbeat the new reader. It
                        should generally be a shorter period of time than the
                        normal heartbeat period in order to help the new
                        reader catch up.
                        -->
                        <late_joiner_heartbeat_period>
                            <!-- 10 milliseconds: -->
                            <sec>0</sec>
                            <nanosec>10000000</nanosec>
                        </late_joiner_heartbeat_period>

                        <!--
                        The number of times a reliable writer will send a
                        heartbeat to a reader without receiving a response
                        before it will consider the reader to be inactive and
                        no longer await acknowledgements before discarding
                        sent data.

                        On a non-real-time operating system like Windows or
                        Linux, a poorly behaving process could monopolize the
                        CPU for several seconds. Therefore, in many cases a
                        value that yields a "grace period" of several seconds
                        is a good choice.
                        -->
                        <max_heartbeat_retries>500</max_heartbeat_retries>

                        <!--
                        When a DataWriter receives a negative acknowledgement
                        (NACK) from a DataReader for a particular data sample,
                        it will send a repair packet to that reader.

                        The amount of time the writer waits between receiving
                        the NACK and sending the repair will be a random
                        value in between the minimum and maximum values
                        specified here. Narrowing the range, and shifting it
                        towards zero, will make the writer more reactive.
                        However, by leaving some delay, you increase the
                        chances that the writer will learn of additional
                        readers that missed the same data, in which case it
                        will be able to send a single multicast repair
                        instead of multiple unicast repairs, thereby using
                        the available network bandwidth more efficiently. The
                        higher the number of readers on the topic, and the
                        greater the load on your network, the more you should
                        consider specifying a range here.
                        -->
                        <min_nack_response_delay>
                            <sec>0</sec>
                            <nanosec>0</nanosec>
                        </min_nack_response_delay>
                        <max_nack_response_delay>
                            <sec>0</sec>
                            <nanosec>0</nanosec>
                        </max_nack_response_delay>
                        <!--
                        Set the maximum number of unacknowedged samples 
                        (batches) in the DataWriter's queue equal to the max 
                        number of batches, to limit how far ahead a writer can 
                        get ahead of its potentially slow readers.
                        -->
                        <min_send_window_size>32</min_send_window_size>
                        <max_send_window_size>32</max_send_window_size>
                    </rtps_reliable_writer>
                </protocol>
				
				 <!--
                When sending very many small data samples, the efficiency of
                the network can be increased by batching multiple samples
                together in a single protocol-level message (usually
                corresponding to a single network datagram). Batching can
                offer very substantial throughput gains, but often at the
                expense of latency, although in some configurations, the
                latency penalty can be very small or even zero - even
                negative.
                -->
                <batch>
                    <enable>true</enable>

                    <!--
                    Batches can be "flushed" to the network based on a
                    maximum size. This size can be based on the total number
                    of bytes in the accumulated data samples and/or the number
                    of samples. Whenever the first of these limits is reached,
                    the batch will be flushed.
                    -->
                    <max_data_bytes>2500</max_data_bytes>
                    <max_meta_data_bytes>2500</max_meta_data_bytes>
                    <max_samples>LENGTH_UNLIMITED</max_samples>

                    <!--
                    Batches can be flushed to the network based on an elapsed
                    time.
                    -->
                    <max_flush_delay>
                        <sec>0</sec>
                        <nanosec>50000000</nanosec>
                    </max_flush_delay>

                    <!--
                    The middleware will associate a source timestamp with a
                    batch when it is started. The duration below indicates
                    the amount of time that may pass before the middleware
                    will insert an additional timestamp into the middle of an
                    existing batch.
        
                    Shortening this duration can give readers increased
                    timestamp resolution if they require that. However,
                    lengthening this duration decreases the amount of
                    meta-data on the network, potentially improving
                    throughput, especially if the data samples are very small.
                    If this delay is set to an infinite time period,
                    timestamps will be inserted only once per batch, and
                    furthermore the middleware will not need to check the
                    time with each sample in the batch, reducing the amount
                    of computation on the send path and potentially improving
                    both latency and throughput performance.
                    -->
                    <source_timestamp_resolution>
                        <sec>DURATION_INFINITE_SEC</sec>
                        <nanosec>DURATION_INFINITE_NSEC</nanosec>
                    </source_timestamp_resolution>
                </batch>
			</datawriter_qos>
			

			<!-- QoS used to configure the data reader created in the example code -->				
			<datareader_qos>
				<subscription_name>
					<name>TurbineDataReader</name>
				</subscription_name>
				
				<!-- Vi er ikke interesserede i historik -->
				<history>
					<kind>KEEP_LAST_HISTORY_QOS</kind>
					<depth>1</depth>
				</history>
                <!--
                The following parameters tune the behavior of the reliability
                protocol. Setting them is not required in order to achieve
                strict reliability but is beneficial from a performance
                standpoint. 
                -->

                <protocol>
                    <rtps_reliable_reader>
                        <!--
                        When the DataReader receives a heartbeat from a
                        DataWriter (indicating (a) that the DataWriter still
                        exists on the network and (b) what sequence numbers
                        it has published), the following parameters indicate
                        how long it will wait before replying with a positive
                        (assuming they aren't disabled) or negative
                        acknowledgement.

                        The time the reader waits will be a random duration
                        in between the minimum and maximum values. Narrowing
                        this range, and shifting it towards zero, will make
                        the system more reactive. However, it will increase
                        the chance of (N)ACK spikes. The higher the number of
                        readers on the topic, and the greater the load on
                        your network, the more you should consider specifying
                        a range here.
                        -->
                        <min_heartbeat_response_delay>
                            <sec>0</sec>
                            <nanosec>0</nanosec>
                        </min_heartbeat_response_delay>
                        <max_heartbeat_response_delay>
                            <sec>0</sec>
                            <nanosec>0</nanosec>
                        </max_heartbeat_response_delay>
                    </rtps_reliable_reader>
                </protocol>
			</datareader_qos>
        </qos_profile>
	</qos_library>
</dds>
